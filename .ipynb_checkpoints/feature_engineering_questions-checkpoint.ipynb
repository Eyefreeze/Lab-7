{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7 - Textual Data Analytics\n",
    "Complete the code with TODO tag.\n",
    "## 1. Feature Engineering\n",
    "In this exercise we will understand the functioning of TF/IDF ranking. Implement the feature engineering and its application, based on the code framework provided below.\n",
    "\n",
    "First we use textual data from Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2819\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>849636868052275200</td>\n",
       "      <td>2017-04-05 14:56:29</td>\n",
       "      <td>b'And so the robots spared humanity ... https:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>848988730585096192</td>\n",
       "      <td>2017-04-03 20:01:01</td>\n",
       "      <td>b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>848943072423497728</td>\n",
       "      <td>2017-04-03 16:59:35</td>\n",
       "      <td>b'@waltmossberg @mims @defcon_5 Et tu, Walt?'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>848935705057280001</td>\n",
       "      <td>2017-04-03 16:30:19</td>\n",
       "      <td>b'Stormy weather in Shortville ...'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>848416049573658624</td>\n",
       "      <td>2017-04-02 06:05:23</td>\n",
       "      <td>b\"@DaveLeeBBC @verge Coal is dying due to nat ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id           created_at  \\\n",
       "0  849636868052275200  2017-04-05 14:56:29   \n",
       "1  848988730585096192  2017-04-03 20:01:01   \n",
       "2  848943072423497728  2017-04-03 16:59:35   \n",
       "3  848935705057280001  2017-04-03 16:30:19   \n",
       "4  848416049573658624  2017-04-02 06:05:23   \n",
       "\n",
       "                                                text  \n",
       "0  b'And so the robots spared humanity ... https:...  \n",
       "1  b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...  \n",
       "2      b'@waltmossberg @mims @defcon_5 Et tu, Walt?'  \n",
       "3                b'Stormy weather in Shortville ...'  \n",
       "4  b\"@DaveLeeBBC @verge Coal is dying due to nat ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "from scipy import spatial\n",
    "from operator import itemgetter\n",
    "\n",
    "data = pd.read_csv('elonmusk_tweets.csv')\n",
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Text Normalization\n",
    "Now we need to normalize text by stemming, tokenizing, and removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "import pprint \n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "emotes = r\"\"\"\n",
    "    (?:\n",
    "        [:=;]\n",
    "        [oO\\-]?\n",
    "        [D\\)\\]\\(\\]/\\\\OpP]\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str = [emotes,\n",
    "            r'<[^>]+>', # HTML tags\n",
    "            r'(?:@[\\w_]+)', # @-mentions\n",
    "            r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "            r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "\n",
    "            r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "            r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "            r'(?:[\\w_]+)', # other words\n",
    "            r'(?:\\S)' # anything else        \n",
    "            ]\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emotes+'$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "def tokenize(a):\n",
    "    return tokens_re.findall(a)\n",
    "\n",
    "def normalize(x):\n",
    "    new_result = []\n",
    "    result = tokenize(x.lower())\n",
    "    for idx, word in enumerate(result, start=1):\n",
    "        if len(word) > 1:\n",
    "            new_result.append(word)         \n",
    "    return new_result\n",
    "\n",
    "\n",
    "original_documents = [x.strip() for x in data['text']] \n",
    "documents = [normalize(x) for x in original_documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see that the normalization is still not perfect. Please feel free to improve upon (OPTIONAL), e.g. https://marcobonzanini.com/2015/03/09/mining-twitter-data-with-python-part-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Implement TF-IDF\n",
    "Now you need to implement TF-IDF, including creating the vocabulary, computing term frequency, and normalizing by tf-idf weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tesla', 272)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['x92', 'europe', 'latest', 'yup', 'hit']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten all the documents\n",
    "flat_list = [word for doc in documents for word in doc]\n",
    "\n",
    "# TODO: remove stop words from the vocabulary\n",
    "words = [word for word in flat_list if not word in stopwords.words('english')]\n",
    "\n",
    "# TODO: we take the 500 most common words only\n",
    "counts = Counter(words)\n",
    "counts = sorted(counts.items(), key=lambda x: x[1])\n",
    "vocabulary = counts[-500:]\n",
    "print([x for x in vocabulary if x[0] == 'tesla'])\n",
    "vocabulary = [x[0] for x in vocabulary]\n",
    "assert len(vocabulary) == 500\n",
    "\n",
    "# vocabulary.sort()\n",
    "vocabulary[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['exactly', \"that's\", 'xa6', 'tesla', 'x80', 'xe2'], dtype='<U16'),\n",
       " array([1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tf(vocabulary, documents):\n",
    "    matrix = [0] * len(documents)\n",
    "    for i, document in enumerate(documents):\n",
    "        counts = Counter(document)\n",
    "        matrix[i] = [0] * len(vocabulary)\n",
    "        for j, term in enumerate(vocabulary):\n",
    "            matrix[i][j] = counts[term]\n",
    "    return matrix\n",
    "\n",
    "tf = tf(vocabulary, documents)\n",
    "np.array(vocabulary)[np.where(np.array(tf[1]) > 0)], np.array(tf[1])[np.where(np.array(tf[1]) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x92': 5.641552398120067, 'europe': 5.641552398120067, 'latest': 5.641552398120067, 'yup': 5.641552398120067, 'hit': 5.746912913777893, 'orders': 5.641552398120067, 'cargo': 5.641552398120067, 'put': 5.641552398120067, 'line': 5.641552398120067, 'consumer': 5.641552398120067, 'machine': 5.746912913777893, 'cause': 5.641552398120067, 'built': 5.641552398120067, 'since': 5.641552398120067, 'least': 5.641552398120067, 'planet': 5.641552398120067, 'house': 5.641552398120067, 'favorite': 5.641552398120067, 'level': 5.641552398120067, 'beyond': 5.641552398120067, \"b'spacex\": 5.641552398120067, 'case': 5.641552398120067, 'fuel': 5.641552398120067, \"b'looks\": 5.641552398120067, 'tested': 5.641552398120067, 'hw2': 5.746912913777893, 'half': 5.641552398120067, 'meeting': 5.641552398120067, 'went': 5.641552398120067, 'competition': 5.641552398120067, 'reading': 5.746912913777893, 'near': 5.641552398120067, '@theeconomist': 5.641552398120067, 'ny': 5.746912913777893, 'hold': 5.641552398120067, 'x85': 7.250990310554167, 'date': 5.641552398120067, 'clear': 5.641552398120067, 'job': 5.641552398120067, 'name': 5.641552398120067, '@westcoastbill': 5.641552398120067, 'appreciated': 5.641552398120067, 'oxygen': 5.641552398120067, \"b'we\": 5.641552398120067, '#tesla': 5.641552398120067, 'ride': 5.641552398120067, 'fun': 5.641552398120067, 'view': 5.641552398120067, 'leg': 5.641552398120067, 'crew': 5.641552398120067, '@theonion': 5.641552398120067, '@openai': 5.546242218315742, '@badastronomer': 5.546242218315742, '20': 5.546242218315742, 'mode': 5.546242218315742, 'x86': 5.546242218315742, 'landed': 5.546242218315742, 'orbital': 5.641552398120067, 'times': 5.546242218315742, 'china': 5.546242218315742, 'norway': 5.641552398120067, 'x98': 6.8455252024460025, 'white': 5.546242218315742, 'talk': 5.546242218315742, 'hardware': 5.546242218315742, 'self-driving': 5.546242218315742, 'forward': 5.546242218315742, 'welcome': 5.546242218315742, 'bit': 5.546242218315742, 'building': 5.546242218315742, 'give': 5.546242218315742, 'liftoff': 5.546242218315742, 'complex': 5.641552398120067, 'sounds': 5.546242218315742, '@mactechgenius': 5.546242218315742, 'chance': 5.641552398120067, 'cover': 5.546242218315742, 'fine': 5.546242218315742, 'come': 5.546242218315742, 'ready': 5.546242218315742, 'possible': 5.546242218315742, 'second': 5.546242218315742, 'mass': 5.546242218315742, 'heard': 5.546242218315742, 'action': 5.546242218315742, 'ft': 5.746912913777893, 'taken': 5.546242218315742, '45': 5.641552398120067, 'price': 5.546242218315742, 'takes': 5.546242218315742, 'complete': 5.546242218315742, 'completed': 5.546242218315742, 'reports': 5.546242218315742, \"b'i\": 5.546242218315742, 'x93': 5.864695949434276, \"b'this\": 5.546242218315742, 'science': 5.546242218315742, 'zero': 5.546242218315742, 'everyone': 5.546242218315742, 'state': 5.546242218315742, \"b'my\": 5.546242218315742, 'per': 5.641552398120067, 'term': 5.641552398120067, 'legs': 5.546242218315742, \"b'about\": 5.546242218315742, 'grasshopper': 5.546242218315742, 'head': 5.546242218315742, 'swap': 5.641552398120067, 'dealers': 5.546242218315742, 'crazy': 5.641552398120067, 'bring': 5.459230841326113, 'def': 5.459230841326113, 'gt': 5.546242218315742, 'opens': 5.459230841326113, ':\\\\': 5.459230841326113, \"b'if\": 5.459230841326113, 'deep': 5.459230841326113, 'country': 5.459230841326113, 'though': 5.459230841326113, 'gen': 5.459230841326113, 'burn': 5.459230841326113, 'plan': 5.459230841326113, 'gets': 5.459230841326113, 'direct': 5.459230841326113, 'idea': 5.546242218315742, 'pressure': 5.459230841326113, 'btw': 5.459230841326113, 'another': 5.459230841326113, 'human': 5.459230841326113, 'order': 5.459230841326113, 'musk': 5.459230841326113, 'firing': 5.459230841326113, 'tmrw': 5.459230841326113, 'says': 5.459230841326113, 'allow': 5.459230841326113, 'interesting': 5.459230841326113, 'customers': 5.459230841326113, '12': 5.459230841326113, 'cost': 5.459230841326113, 'media': 5.459230841326113, 'product': 5.459230841326113, 'set': 5.459230841326113, 'seems': 5.459230841326113, 'using': 5.459230841326113, 'short': 5.459230841326113, \"b'rocket\": 5.459230841326113, 'reentry': 5.459230841326113, \"b'dragon\": 5.459230841326113, 'ocean': 5.459230841326113, 'http': 5.459230841326113, 'humanity': 5.379188133652576, 'deployed': 5.379188133652576, 'window': 5.379188133652576, 'away': 5.379188133652576, 'problem': 5.379188133652576, 'series': 5.459230841326113, 'hope': 5.379188133652576, 'works': 5.379188133652576, 'likely': 5.379188133652576, 'webcast': 5.379188133652576, 'th': 5.546242218315742, 'thought': 5.379188133652576, 'bad': 5.379188133652576, 'shows': 5.379188133652576, 'force': 5.379188133652576, 'radar': 5.379188133652576, 'recommend': 5.379188133652576, 'close': 5.379188133652576, 'return': 5.459230841326113, 'velocity': 5.379188133652576, 'details': 5.379188133652576, \"b'launch\": 5.379188133652576, 'fully': 5.305080161498854, 'trying': 5.305080161498854, 'worth': 5.305080161498854, 'three': 5.379188133652576, 'yet': 5.305080161498854, \"i've\": 5.305080161498854, 'advanced': 5.379188133652576, 'sense': 5.305080161498854, 'gigafactory': 5.305080161498854, 'weekend': 5.305080161498854, '11': 5.305080161498854, 'morning': 5.305080161498854, 'person': 5.305080161498854, 'post': 5.305080161498854, 'try': 5.305080161498854, \"tesla's\": 5.305080161498854, 'tonight': 5.305080161498854, 'water': 5.379188133652576, 'satellites': 5.305080161498854, '@vicentes': 5.305080161498854, \"b'great\": 5.305080161498854, 'size': 5.305080161498854, 'coast': 5.546242218315742, 'thing': 5.236087290011903, 'https': 5.236087290011903, 'piece': 5.236087290011903, 'km': 5.305080161498854, 'technology': 5.236087290011903, \"b'am\": 5.236087290011903, 'satellite': 5.236087290011903, 'control': 5.305080161498854, 'free': 5.236087290011903, 'without': 5.236087290011903, 'might': 5.236087290011903, 'two': 5.236087290011903, 'spacecraft': 5.236087290011903, 'charge': 5.236087290011903, 'service': 5.236087290011903, 'agree': 5.236087290011903, 'several': 5.236087290011903, 'later': 5.305080161498854, 'owners': 5.236087290011903, '40': 5.236087290011903, 'engines': 5.305080161498854, 'goes': 5.236087290011903, 'road': 5.236087290011903, '@id_aa_carmack': 5.236087290011903, 'must': 5.305080161498854, 'roof': 5.379188133652576, 'unveil': 5.236087290011903, \"b'good\": 5.236087290011903, 'booster': 5.236087290011903, 'got': 5.236087290011903, '50': 5.236087290011903, 'auto': 5.305080161498854, 'sales': 5.236087290011903, 'public': 5.379188133652576, 'little': 5.171548768874331, 'hours': 5.171548768874331, 'systems': 5.171548768874331, 'vertical': 5.171548768874331, 'needs': 5.171548768874331, 'amazing': 5.171548768874331, 'version': 5.305080161498854, 'fast': 5.171548768874331, 'motor': 5.236087290011903, 'months': 5.236087290011903, 'took': 5.171548768874331, 'spaceship': 5.171548768874331, 'open': 5.171548768874331, 'small': 5.171548768874331, 'owner': 5.171548768874331, 'carbon': 5.236087290011903, 'performance': 5.171548768874331, 'record': 5.171548768874331, 'things': 5.171548768874331, 'anything': 5.171548768874331, 'wrong': 5.171548768874331, 'find': 5.171548768874331, '@wired': 5.171548768874331, 'mile': 5.171548768874331, 'every': 5.236087290011903, \"b'will\": 5.171548768874331, 'rear': 5.379188133652576, 'part': 5.305080161498854, 'others': 5.236087290011903, 'texas': 5.171548768874331, 'around': 5.110924147057896, 'moon': 5.110924147057896, 'risk': 5.110924147057896, 'start': 5.110924147057896, 'pack': 5.110924147057896, 'thank': 5.110924147057896, 'used': 5.110924147057896, 'side': 5.110924147057896, '@fredericlambert': 5.110924147057896, 'definitely': 5.110924147057896, 'called': 5.110924147057896, 'never': 5.110924147057896, 'said': 5.110924147057896, 'please': 5.110924147057896, 'sec': 5.236087290011903, 'tax': 5.171548768874331, 'making': 5.110924147057896, 'target': 5.110924147057896, 'far': 5.110924147057896, 'early': 5.110924147057896, 'top': 5.110924147057896, 'night': 5.110924147057896, 'center': 5.110924147057896, 'happy': 5.171548768874331, 'range': 5.053765733217948, 'elon': 5.053765733217948, 'month': 5.053765733217948, 'home': 5.110924147057896, 'story': 5.053765733217948, 'support': 5.053765733217948, 'use': 5.053765733217948, 'say': 5.110924147057896, 'data': 5.053765733217948, 'company': 5.110924147057896, 'solarcity': 5.053765733217948, 'trip': 5.053765733217948, 'news': 5.053765733217948, 'max': 5.110924147057896, 'important': 4.999698511947672, \"b'falcon\": 4.999698511947672, 'upper': 4.999698511947672, 'release': 5.053765733217948, 'less': 5.053765733217948, 'looking': 4.999698511947672, 'announcement': 4.999698511947672, '@space_station': 4.999698511947672, 'mins': 4.999698511947672, 'driving': 4.999698511947672, 'gas': 4.999698511947672, 'low': 4.999698511947672, 'weeks': 4.948405217560121, 'pad': 4.948405217560121, 'ok': 4.948405217560121, 'kids': 4.999698511947672, 'enough': 4.948405217560121, 'hopefully': 4.948405217560121, 'review': 4.948405217560121, 'air': 4.948405217560121, 'safety': 4.999698511947672, 'vehicle': 4.948405217560121, 'attempt': 4.948405217560121, \"world's\": 4.948405217560121, 'heavy': 4.89961505339069, 'show': 4.89961505339069, 'hard': 4.89961505339069, 'ship': 4.89961505339069, 'land': 4.999698511947672, 'weather': 4.853095037755796, \"can't\": 4.853095037755796, 'course': 4.853095037755796, '100': 4.853095037755796, 'design': 4.853095037755796, 'always': 4.89961505339069, 'vs': 4.89961505339069, 'days': 4.853095037755796, 'supercharger': 4.853095037755796, 'earth': 4.89961505339069, 'big': 4.853095037755796, 'droneship': 4.89961505339069, 'real': 4.853095037755796, 'speed': 4.853095037755796, 'done': 4.853095037755796, 'engine': 4.808643275184963, 'may': 4.808643275184963, 'mph': 4.948405217560121, 'hyperloop': 4.948405217560121, \"b'the\": 4.808643275184963, 'super': 4.808643275184963, 'made': 4.89961505339069, 'california': 4.808643275184963, 'fire': 4.808643275184963, 'point': 4.766083660766167, 'already': 4.766083660766167, '60': 4.766083660766167, 'electric': 4.766083660766167, 'tomorrow': 4.766083660766167, 'take': 4.766083660766167, 'help': 4.808643275184963, 'working': 4.7252616662459115, 'st': 4.766083660766167, 'makes': 4.7252616662459115, 'something': 4.7252616662459115, 'canaveral': 4.7252616662459115, 'see': 4.766083660766167, 'life': 4.7252616662459115, 'read': 4.7252616662459115, 'thrust': 4.7252616662459115, 'nhttp': 4.7252616662459115, 'full': 4.686040953092631, 'update': 4.766083660766167, 'world': 4.686040953092631, 'power': 4.686040953092631, 'want': 4.766083660766167, '#dragon': 4.686040953092631, 'cool': 4.686040953092631, 'la': 4.686040953092631, 'et': 4.7252616662459115, 'watch': 4.686040953092631, 'battery': 4.648300625109783, 'x9d': 4.853095037755796, 'look': 4.648300625109783, 'true': 4.648300625109783, 'almost': 4.648300625109783, 'know': 4.648300625109783, 'sure': 4.648300625109783, \"b'just\": 4.648300625109783, 'even': 4.648300625109783, 'end': 4.648300625109783, 'exactly': 4.611932980938908, 'due': 4.611932980938908, 'drive': 4.611932980938908, 'needed': 4.648300625109783, 'cape': 4.611932980938908, 'looks': 4.576841661127639, 'could': 4.576841661127639, 'energy': 4.611932980938908, 'orbit': 4.611932980938908, 'system': 4.611932980938908, 'awesome': 4.576841661127639, 'via': 4.576841661127639, 'well': 4.5429401094519575, 'ai': 4.611932980938908, 'mission': 4.510150286628966, 'climate': 4.5429401094519575, 'going': 4.478401588314386, 'team': 4.510150286628966, 'work': 4.510150286628966, 'long': 4.478401588314386, 'day': 4.5429401094519575, 'x99s': 4.510150286628966, 'probably': 4.447629929647633, 'years': 4.510150286628966, 'last': 4.447629929647633, 'software': 4.447629929647633, 'miles': 4.447629929647633, 'x9c': 4.510150286628966, 'coming': 4.417776966497951, 'production': 4.417776966497951, 'think': 4.447629929647633, \"i'm\": 4.478401588314386, 'change': 4.447629929647633, 'yeah': 4.417776966497951, '30': 4.510150286628966, \"that's\": 4.360618552658003, \"b'model\": 4.360618552658003, 'station': 4.360618552658003, 'many': 4.3887894296246985, 'actually': 4.306551331387727, 'maybe': 4.306551331387727, 'still': 4.306551331387727, 'get': 4.306551331387727, 'video': 4.280575844984466, 'article': 4.280575844984466, 'love': 4.255258037000176, '10': 4.306551331387727, 'high': 4.255258037000176, 'make': 4.280575844984466, 'ever': 4.280575844984466, 'live': 4.280575844984466, 'lot': 4.280575844984466, 'future': 4.2305654244098045, 'soon': 4.2064678728307445, 'back': 4.2064678728307445, 'better': 4.1829373754205506, 'way': 4.2305654244098045, 'pm': 4.255258037000176, 'us': 4.137475001343793, 'stage': 4.255258037000176, 'go': 4.115496094625017, 'today': 4.072936480206222, 'right': 4.115496094625017, '@nasa': 4.072936480206222, 'need': 4.052317193003486, 'week': 4.093989889404054, 'really': 4.072936480206222, 'also': 4.032114485685967, 'solar': 4.072936480206222, \"b'tesla\": 4.032114485685967, 'much': 4.012311858389786, 'flight': 4.032114485685967, 'best': 4.032114485685967, 'nhttps': 4.012311858389786, 'year': 4.032114485685967, 'mars': 3.992893772532685, 'great': 3.9187858003789633, 'spacex': 3.8666000472083932, 'space': 3.8836944805676934, 'test': 3.9187858003789633, 'autopilot': 3.9010862232795622, 'people': 3.849792928892012, 'one': 3.833263626940801, 'thanks': 3.754482749087687, 'cars': 3.80100276472258, 'would': 3.754482749087687, ':)': 3.667471372098057, 'new': 3.667471372098057, 'time': 3.6400723979099427, 'dragon': 3.6536780499657215, 'co': 3.6134041508277814, 'landing': 3.681457614072797, 'first': 3.600332069260429, 'next': 3.537418243849859, '@elonmusk': 3.478229372459529, 'falcon': 3.466800676635906, ':/': 3.455501121381973, 'rt': 3.3490176409795227, 'yes': 3.3490176409795227, 'car': 3.3797892996462764, 'like': 3.225638619819018, 'rocket': 3.140116446380856, 'good': 3.131953135741695, 'launch': 3.123855925509076, 'xa6': 3.0388627126756833, '@teslamotors': 2.8689636758802854, '@spacex': 2.7908458966163336, 'amp': 2.8024739346114527, 'model': 2.6558704604195773, 'tesla': 2.356888832713863, 'x80': 2.5734994629864496, 'xe2': 2.5324914392590725, \"b'rt\": 1.9703278792448513}\n"
     ]
    }
   ],
   "source": [
    "def idf(vocabulary, documents):\n",
    "    \"\"\"TODO: compute IDF, storing values in a dictionary\"\"\"\n",
    "    idf = dict()\n",
    "    C = len(documents)\n",
    "    \n",
    "    for word in vocabulary:\n",
    "        Ct = 0\n",
    "        for document in documents:\n",
    "            if word in document:\n",
    "                Ct += 1\n",
    "        idf[word] = math.log(C/Ct)\n",
    "    \n",
    "    return idf\n",
    "\n",
    "idf = idf(vocabulary, documents)\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['exactly', \"that's\", 'xa6', 'tesla', 'x80', 'xe2'], dtype='<U16'),\n",
       " array([4.61193298, 4.36061855, 3.03886271, 2.35688883, 2.57349946,\n",
       "        2.53249144]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize(document, vocabulary, idf):\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * counts[term]\n",
    "    return vector\n",
    "\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]\n",
    "np.array(vocabulary)[np.where(np.array(document_vectors[1]) > 0)], np.array(document_vectors[1])[np.where(np.array(document_vectors[1]) > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Compare the results with the reference implementation of scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the scikit-learn library. As you can see that, the way we do text normalization affects the result. Feel free to further improve upon (OPTIONAL), e.g. https://stackoverflow.com/questions/36182502/add-stemming-support-to-countvectorizer-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('http', 163.54366542841234), ('https', 151.85039944652075), ('rt', 112.61998731390989), ('tesla', 95.96401470715628), ('xe2', 88.20944486346477)]\n",
      "testla 0.3495243100660956\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english', max_features=500)\n",
    "\n",
    "features = tfidf.fit(original_documents)\n",
    "corpus_tf_idf = tfidf.transform(original_documents) \n",
    "\n",
    "sum_words = corpus_tf_idf.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in tfidf.vocabulary_.items()]\n",
    "print(sorted(words_freq, key = lambda x: x[1], reverse=True)[:5])\n",
    "print('testla', corpus_tf_idf[1, features.vocabulary_['tesla']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.  Apply TF-IDF for information retrieval\n",
    "We can use the vector representation of documents to implement an information retrieval system. We test with the query $Q$ = \"tesla nasa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 documents\n",
      "0 b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exactly. Tesla is absurdly overvalued if based on the past, but that's irr\\xe2\\x80\\xa6 https://t.co/qQcTqkzgMl\"\n",
      "1 b'Stormy weather in Shortville ...'\n",
      "2 b\"@DaveLeeBBC @verge Coal is dying due to nat gas fracking. It's basically dead.\"\n",
      "3 b\"@Lexxxzis It's just a helicopter in helicopter's clothing\"\n",
      "4 b'Technology breakthrough: turns out chemtrails are actually a message from time-traveling aliens describing the secret of teleportation'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:702: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(v1,v2):\n",
    "    \"\"\"TODO: compute cosine similarity\"\"\"\n",
    "    return 1 - spatial.distance.cosine(v1, v2)\n",
    "\n",
    "def search_vec(query, k, vocabulary, stemmer, document_vectors, original_documents):\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    \n",
    "    # TODO: rank the documents by cosine similarity\n",
    "    scores = list()\n",
    "    for i, dv in enumerate(document_vectors, start=1):\n",
    "        scores.append([cosine_similarity(query_vector, dv), i])\n",
    "    scores = sorted(scores, key=itemgetter(0))\n",
    "    scores[::-1]\n",
    "    print('Top-{0} documents'.format(k))\n",
    "    for i in range(k):\n",
    "        print(i, original_documents[scores[i][1]])\n",
    "\n",
    "query = \"tesla nasa\"\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "vocabulary = [x.lower() for x in vocabulary]\n",
    "search_vec(query, 5, vocabulary, stemmer, document_vectors, original_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the scikit-learn library to do the retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 documents\n",
      "0 b'@ashwin7002 @NASA @faa @AFPAA We have not ruled that out.'\n",
      "1 b\"SpaceX could not do this without NASA. Can't express enough appreciation. https://t.co/uQpI60zAV7\"\n",
      "2 b'@NASA launched a rocket into the northern lights http://t.co/tR2cSeMV'\n",
      "3 b'Whatever happens today, we could not have done it without @NASA, but errors are ours alone and me most of all.'\n",
      "4 b'RT @NASA: Updated @SpaceX #Dragon #ISS rendezvous times: NASA TV coverage begins Sunday at 3:30amET: http://t.co/qrm0Dz4jPE. Grapple at  ...'\n"
     ]
    }
   ],
   "source": [
    "new_features = tfidf.transform([query])\n",
    "\n",
    "cosine_similarities = linear_kernel(new_features, corpus_tf_idf).flatten()\n",
    "related_docs_indices = cosine_similarities.argsort()[::-1]\n",
    "\n",
    "topk = 5\n",
    "print('Top-{0} documents'.format(topk))\n",
    "for i in range(topk):\n",
    "    print(i, original_documents[related_docs_indices[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
